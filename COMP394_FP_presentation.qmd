---
title: "COMP394 Project Presentation"
author: "Charles Batsaikhan, William Acosta Lora, Daisy Chan"
format: 
  revealjs: 
    theme: moon
    chalkboard: true
editor: visual
---

## Introduction and Research Question {.scrollable .smaller}

::: panel-tabset
### Introduction

-   What we’re studying:
    -   Reddit’s r/AmITheAsshole (AITA) forum, where users vote YTA (you’re the asshole), NTA (not the asshole), ESH (everyone sucks here), or NAH (no assholes here).
-   Why it matters
    -   These public “verdicts” shape reputations online and fuel cancel‑culture talk.
-   Our data
    -   Hugging Face dataset by Oguzz07
    -   Posts 900 comments, verdict labels kept, usernames removed for privacy.

### Research Question

-   Research Question:
    -   Testing whether our sentiment is directed towards our original poster (OP) or other parties??
-   How we’ll test it:
    -   BERT Model
    -   Gemini
:::

::: notes
We’re diving into r/AmITheAsshole, a subreddit where people post real‑life dilemmas and get judged by strangers. Each comment carries a clear verdict—you’re the jerk (YTA), you’re not (NTA), or one of two middle‑ground options. That makes AITA a gold‑mine for studying moral judgment online.

Why should we care? Because these verdicts don’t stay on Reddit. They end up in news stories, TikToks, and sometimes real‑world backlash—so understanding how people reach them is important.

We grabbed a public dataset (credits to the user Oguzz07) that includes 900+ comments. We stripped out usernames and links to keep things anonymous.

Our main research question is simple: if we run a standard sentiment model—basically a tool that scores text as positive or negative—will it line up with what the crowd decided? And if it doesn’t, what kinds of words or writing styles throw it off?

To test this, we’ll clean the text, feed it to BERT for sentiment scores, and then check how often those scores match the actual YTA, NTA, ESH, or NAH tags. The accuracy numbers—and the mistakes—will tell us a lot about whether tone alone is enough to read moral judgment online.
:::

## Methods Overview {.scrollable .smaller}

-   **Goal:** Determine whether AITA comment sentiment is directed at the Original Poster (OP) or someone else.
-   **Step 1: Data Mapping**
    -   Used 5 original sentiment labels: *positive, negative, neutral, neutral (positive), neutral (negative)*.
    -   Mapped to two categories:
        -   **OP** = Positive, Neutral (Positive), Neutral
        -   **Other** = Negative, Neutral (Negative)
-   **Step 2: Gemini Predictions**
    -   Prompted Gemini to classify each response: *"Is this criticizing the OP or someone else?"*
    -   Saved Gemini's binary prediction: "OP" or "Other"
-   **Step 3: BERT Model**
    -   Used `bert-base-uncased` in a text classification pipeline
    -   Ran responses through BERT with truncation at 512 tokens
    -   Saved predicted labels: "OP" or "Other"
-   **Step 4: Evaluation**
    -   Compared both models to mapped sentiment labels using:
        -   Confusion matrices
        -   Precision, recall, F1-score
        -   Chi-square test to compare prediction distributions


## Data Collection and Annotation {.scrollable .smaller}

-   **Source:**
    -   Used the [r/AmITheAsshole dataset](https://huggingface.co/datasets/Oguzz07/AmItheAsshole) from Hugging Face
    -   Created by user Oguzz07
    -   Includes: Post text, comment responses, and final subreddit verdicts (YTA, NTA, ESH, NAH)
-   **Volume:**
    -   Over 1 million comments total
    -   Focused on a subset of responses that had been sentiment-labeled
-   **Annotation Strategy:**
    -   Dataset included human-assigned sentiment labels:
        -   *Positive, Negative, Neutral (positive), Neutral (negative), Neutral*
    -   We mapped these into binary classes for target inference:
        -   **OP** = Positive, Neutral (positive), Neutral
        -   **Other** = Negative, Neutral (negative)
    -   Assumption: Sentiment directed at someone in the post reflects the target of criticism
-   **Goal of Annotation:**
    -   Create a “ground truth” for training/evaluating language models
    -   Labels do not directly say “OP” or “Other,” so this mapping is our interpretive layer

## Analysis Methods {.scrollable .smaller}

-   **Binary Classification Task:**
    -   Determine whether a Reddit response targets:
        -   **OP** (Original Poster)
        -   **Other** (another person mentioned)
-   **Model 1: Gemini (Google API)**
    -   Prompted with:\
        \> "Analyze the following Reddit post and determine the sentiment of the response. If the response criticizes the Original Poster (OP), say 'Other'. If the response criticizes someone else (not OP), say 'OP'. Only respond with 'OP' or 'Other'. Here is the Reddit post: "{response}""
    -   Collected responses via API call
    -   Stored predictions as "OP" or "Other"
-   **Model 2: BERT (bert-base-uncased)**
    -   Used Hugging Face’s text classification pipeline
    -   Inputs truncated at 512 tokens
    -   Model outputs label: “LABEL_0” or “LABEL_1” → mapped to "OP" or "Other"
-   **Evaluation:**
    -   Compared both models to ground truth (mapped sentiment labels)
    -   Used:
        -   **Confusion Matrix**
        -   **Precision, Recall, F1-Score**
        -   **Chi-Square Test** to assess if the two models differ significantly in behavior
-   **Tools:**
    -   Python (Pandas, Transformers, SciPy, Seaborn)
    -   Google Colab for experimentation

## Chi-Square Test {.scrollable}

-   **Why a chi-square test?**

    -   To check if Gemini and BERT make significantly different types of predictions
    -   Compares how each model distributes errors (e.g., false positives, false negatives)

-   **Contingency Table Input:**

    | Model  | True OP → OP | True OP → Other | True Other → OP | True Other → Other |
    |--------|--------------|-----------------|-----------------|--------------------|
    | Gemini | 714          | 97              | 47              | 24                 |
    | BERT   | 666          | 201             | 60              | 19                 |

-   **Results:**

    -   Chi² = 37.93\
    -   p-value = 2.92 × 10⁻⁸\
    -   DoF = 3

-   **Interpretation:**

    -   Strong evidence that BERT and Gemini behave differently
    -   Supports our argument that model choice matters in moral sentiment analysis

## Results {.scrollable .smaller}

::: panel-tabset
### Gemini Performance

-   **Confusion Matrix:**
    -   True OP → OP: 714\
    -   True OP → Other: 97\
    -   True Other → OP: 47\
    -   True Other → Other: 24
-   **Metrics:**
    -   Accuracy: **84%**
    -   OP F1-score: **0.91**
    -   Other F1-score: **0.25**
    -   Model tends to **favor OP classification**, underperforms on "Other"

### BERT Performance

-   **Confusion Matrix:**
    -   True OP → OP: 666\
    -   True OP → Other: 201\
    -   True Other → OP: 60\
    -   True Other → Other: 19
-   **Metrics:**
    -   Accuracy: **72%**
    -   OP F1-score: **0.84**
    -   Other F1-score: **0.13**
    -   Weaker than Gemini overall, especially on minority class ("Other")

### Comparison: Chi-Square Test

-   **Statistical Test:** χ² = 37.93, p \< 0.00000003
-   **Interpretation:** Gemini and BERT **distribute predictions significantly differently**
-   **Implication:** Model choice affects who we think is being criticized in AITA — not all LLMs “read” judgment the same way

### Confusion Matrix Heatmaps {.center}

![Confusion matrices for Gemini and BERT](heatmap.png)

- **Gemini (left):** Stronger performance, especially for OP detection  
- **BERT (right):** Weaker precision and recall on “Other” predictions  
- This visual shows how both models skew toward OP classification


:::

## Conclusion and Limitations {.scrollable .smaller}

::: panel-tabset
### Conclusion

-   **Key Insight:**
    -   Sentiment models like BERT and Gemini can approximate moral judgments in r/AmITheAsshole (AITA) discussions.
-   **Model Comparison:**
    -   **Gemini** outperformed BERT in identifying when criticism targets the OP.
    -   **BERT** struggled more with detecting criticism directed at others.
-   **Statistical Significance:**
    -   A chi-square test revealed significant differences in prediction distributions between the two models (χ² = 37.93, p \< 0.00000003).
-   **Implication:**
    -   The choice of language model significantly influences the interpretation of moral judgments in online discourse.

### Limitations

-   **Label Mapping:**
    -   The binary classification of "OP" vs. "Other" was derived from sentiment labels, which may not perfectly capture the target of criticism.
-   **Model Training:**
    -   BERT was used without fine-tuning on AITA-specific data, potentially limiting its effectiveness.
-   **Data Constraints:**
    -   Some responses exceeded the 512-token limit of BERT, leading to truncation and possible loss of context.
-   **Annotation Ambiguity:**
    -   The interpretation of sentiment labels as indicators of criticism direction introduces subjectivity.

### Future Directions

-   **Model Enhancement:**
    -   Fine-tune BERT on AITA-specific data to improve its understanding of nuanced moral judgments.
-   **Expanded Annotation:**
    -   Incorporate more detailed annotations to better capture the directionality of criticism.
-   **Broader Analysis:**
    -   Explore additional linguistic features and context to enhance model predictions.
:::

## Questions? 
## Thanks !!



------------------------------------------------------------------------
