---
title: "COMP394 Project Presentation"
author: "Charles Batsaikhan, William Acosta Lora, Daisy Chan"
format: 
  revealjs: 
    theme: moon
    chalkboard: true
editor: visual
---

## Introduction and Research Question {.scrollable .smaller}

::: panel-tabset
### Introduction

-   What we’re studying:
    -   Reddit’s r/AmITheAsshole (AITA) forum, where users vote YTA (you’re the asshole), NTA (not the asshole), ESH (everyone sucks here), or NAH (no assholes here).
-   Why it matters
    -   These public “verdicts” shape reputations online and fuel cancel‑culture talk.
-   Our data
    -   Hugging Face dataset by Oguzz07
    -   Posts 900 comments, verdict labels kept, usernames removed for privacy.

### Research Question

-   Research Question:
    -   Testing whether our sentiment is directed towards our original poster (OP) or other parties?
-   How we’ll test it:
    -   BERT Model
    -   Gemini
:::

::: notes

Daisy+ William
We’re diving into r/AmITheAsshole, a subreddit where people post real‑life dilemmas and get judged by strangers. Each comment carries a clear verdict—you’re the jerk (YTA), you’re not (NTA), or one of two middle‑ground options. That makes AITA a gold‑mine for studying moral judgment online.

Why should we care? Because these verdicts don’t stay on Reddit. They end up in news stories, TikToks, and sometimes real‑world backlash—so understanding how people reach them is important.

Our dataset is from Hugging Face and includes 900+ posts and labeled comments. We cleaned it and focused on responses that had clear sentiment labels. Our core question: can we predict who’s being criticized in a comment? Is it the original poster (OP), or someone else?

To test this, we compare two tools: Gemini (a large language model via API) and BERT (a popular open-source NLP model). This sets the stage for analyzing how each interprets tone and direction of criticism.
:::

## Data Collection and Annotation {.scrollable .smaller}

-   **Source:**
    -   Used the [r/AmITheAsshole dataset](https://huggingface.co/datasets/Oguzz07/AmItheAsshole) from Hugging Face
    -   Created by user Oguzz07
    -   Includes: Post text, comment responses, and final subreddit verdicts (YTA, NTA, ESH, NAH)
-   **Volume:**
    -   Over 900+ comments total
    -   Focused on a subset of responses that had been sentiment-labeled
-   **Annotation Strategy:**
    -   Constructed a rubric of sentiment on a Positive-Negative scale
        -   *Positive, Negative, Neutral (positive), Neutral (negative), Neutral*
    -   Execution: We each annotated 50 responses by hand (150 total), and submitted the remaining 750+ entries to Gemini with our predefined labels to finish annotation process
        -   Prompt used:

            "Given the following Reddit comment from the r/AmITheAsshole subreddit, classify its sentiment based on the following rubric:

            -   **Positive**: Clearly supportive, kind, or affirming tone.

                **Negative**: Clearly critical, hostile, or harsh in tone.

                **Neutral (positive)**: Neutral in content, but leans slightly supportive or understanding.

                **Neutral (negative)**: Neutral in content, but leans slightly critical or disapproving.

                **Neutral**: Fully neutral in tone and content

            Classify the sentiment of the comment accordingly and return only the label."
    -   We mapped these into binary classes for target inference:
        -   **OP** = Positive, Neutral (positive), Neutral
        -   **Other** = Negative, Neutral (negative)
    -   Assumption: Sentiment directed at someone in the post reflects the target of criticism
-   **Goal of Annotation:**
    -   Create a “ground truth” for training/evaluating language models
    -   Labels do not directly say “OP” or “Other,” so this mapping is our interpretive layer

::: notes
Daisy+ William
Let’s talk about our data.

We used the r/AmITheAsshole dataset from Hugging Face, created by Oguzz07. It includes the post content, user comments, and final subreddit verdicts. Our focus was on the comments — specifically those that already had sentiment labels.

We created an annotation scheme for a scale of sentiment from positive to negative- including 5 levels within that scale. The nuanced categories like “neutral (positive)” or “neutral (negative).” That granularity is great for analyzing tone.

Using this scheme, we each hand-annotated 50 entries of responses each, cumulating to 150 labelled responses. For the remaining 750+ entries, we input them and our scheme into Gemini with the prompt onscreen.

But since there was no explicit label for “who” the comment targeted, we introduced an interpretive layer. We assumed that if a comment is positive or neutral, it likely supports the OP; if it’s negative or neutral-negative, it’s likely criticizing someone else.

This mapping let us create a supervised task where we could evaluate how well language models inferred that directionality of sentiment — toward OP or another party.
:::


## Methods Overview {.scrollable .smaller}

-   **Goal:** Determine whether AITA comment sentiment is directed at the Original Poster (OP) or someone else.
-   **Step 1: Data Mapping**
    -   Used 5 sentiment labels: *positive, negative, neutral, neutral (positive), neutral (negative)*.
    -   Mapped to two categories:
        -   **OP** = Positive, Neutral (Positive), Neutral
        -   **Other** = Negative, Neutral (Negative)
-   **Step 2: Gemini Predictions**
    -   Prompted Gemini to classify each response: *"Is this criticizing the OP or someone else?"*
    -   Saved Gemini's binary prediction: "OP" or "Other"
-   **Step 3: BERT Model**
    -   Used `bert-base-uncased` in a text classification pipeline
    -   Ran responses through BERT with truncation at 512 tokens
    -   Saved predicted labels: "OP" or "Other"
- **Unsupervised Topic Check (LDA)**
  - Ran two LDA setups:
    1. 5‑topic model *with* stop‑words (baseline sanity‑check)  
    2. 5‑ and 10‑topic models *without* stop‑words (meaningful themes)
-   **Step 4: Evaluation**
    -   Compared both models to mapped sentiment labels using:
        -   Confusion matrices
        -   Precision, recall, F1-score
        -   Chi-square test to compare prediction distributions

::: notes
Charles
Here’s how we structured our approach.

First, we mapped all sentiment labels into binary classes. Positive, neutral-positive, and neutral were assumed to refer to the OP — since those sentiments tend to be supportive or neutral toward the main poster. Negative and neutral-negative were mapped to “Other,” assuming criticism was directed elsewhere.

Then we ran two models:

-   Gemini was prompted using clear instructions to classify whether a comment criticizes the OP or not.
-   BERT was used as a pretrained model for binary classification. It doesn’t understand Reddit rules out of the box, so it’s more of a baseline.

Finally, we evaluated model predictions against our mapped labels using confusion matrices and classification metrics. To check for statistical significance between model behaviors, we used a chi-square test.
:::


## Analysis Methods {.scrollable .smaller}

-   **Binary Classification Task:**
    -   Determine whether a Reddit response targets:
        -   **OP** (Original Poster)
        -   **Other** (another person mentioned)
-   **Model 1: Gemini (Google API)**
    -   Prompted with:\
        \> "Analyze the following Reddit post and determine the sentiment of the response. If the response criticizes the Original Poster (OP), say 'Other'. If the response criticizes someone else (not OP), say 'OP'. Only respond with 'OP' or 'Other'. Here is the Reddit post: "{response}""
    -   Collected responses via API call
    -   Stored predictions as "OP" or "Other"
-   **Model 2: BERT (bert-base-uncased)**
    -   Used Hugging Face’s text classification pipeline
    -   Inputs truncated at 512 tokens
    -   Model outputs label: “LABEL_0” or “LABEL_1” → mapped to "OP" or "Other"
-   **Evaluation:**
    -   Compared both models to ground truth (mapped sentiment labels)
    -   Used:
        -   **Confusion Matrix**
        -   **Precision, Recall, F1-Score**
        -   **Chi-Square Test** to assess if the two models differ significantly in behavior
        -   **LDA Model**
-   **Tools:**
    -   Python (Pandas, Transformers, SciPy)
    -   Google Colab for experimentation

::: notes
Charles+ William
Now let’s get into our modeling setup.

We framed the task as binary classification: each Reddit comment either targets the OP or someone else. It’s simple but lets us explore how models process moral and interpersonal cues.

We used two tools: - Gemini, accessed via Google’s API, where we gave it a strict prompt and recorded whether it said “OP” or “Other.” - BERT, a widely used pretrained model, which we ran using the Hugging Face pipeline. We truncated inputs at 512 tokens to meet the model’s length limit.

Both models returned predictions, which we compared to our mapped labels. We calculated standard performance metrics: accuracy, precision, recall, and F1-score.

Then, we used a chi-square test to ask: do Gemini and BERT differ meaningfully in how they distribute their predictions? That helps us assess if model choice changes what moral reading we get from a comment.
:::

## Chi-Square Test {.scrollable}

-   **Why a chi-square test?**

    -   To check if Gemini and BERT make significantly different types of predictions
    -   Compares how each model distributes errors (e.g., false positives, false negatives)


::: notes

:::

## Results {.scrollable .smaller}

::: panel-tabset
### Gemini Performance

-   **Confusion Matrix:**
    -   True OP → OP: 714\
    -   True OP → Other: 97\
    -   True Other → OP: 47\
    -   True Other → Other: 24
-   **Metrics:**
    -   Accuracy: **84%**
    -   OP F1-score: **0.91**
    -   Other F1-score: **0.25**
    -   Model tends to **favor OP classification**, underperforms on "Other"

### BERT Performance

-   **Confusion Matrix:**
    -   True OP → OP: 666\
    -   True OP → Other: 201\
    -   True Other → OP: 60\
    -   True Other → Other: 19
-   **Metrics:**
    -   Accuracy: **72%**
    -   OP F1-score: **0.84**
    -   Other F1-score: **0.13**
    -   Weaker than Gemini overall, especially on minority class ("Other")
    
### LDA Topic Exploration


Baseline (5 Topics, with stop-words)

Dominated by common function words (you, the, to, and).

No meaningful topics emerged.


5-Topic Model (stop-words removed)


| **Topic** | **Top Keywords**                    | **Interpretation**         |
| --------- | ----------------------------------- | -------------------------- |
| 1         | wife, child, daughter, son, parents | Parenting / Kids           |
| 2         | money, house, job, pay              | Finances & Housing         |
| 3         | family, husband, wife, mother, food | OP vs Spouse & In-laws     |
| 4         | sorry, better, want, life, good     | Self-improvement / Apology |
| 5         | brother, sister, parents, wedding   | Siblings & Weddings        |

10-Topic Model (stop words removed)


| **Topic** | **Top Keywords**                | **Interpretation**                   |
| --------- | ------------------------------- | ------------------------------------ |
| 1         | wife, daughter, dad, discipline | Parental favoritism / Discipline     |
| 2         | money, job, kids, cost-sharing  | Financial Responsibility             |
| 3         | mom, wedding, pay, funding      | Event Payments & Parental Funding    |
| 4         | better, life, relationship      | Self-improvement & Leaving Toxicity  |
| 5         | brother, parents, fairness      | Sibling Fairness & Childcare Burdens |
| 6         | divorce, custody, awards        | Updates & Legal Family Issues        |
| 7         | friend, happy, asshole          | Friendship & Social Conflicts        |
| 8         | right, eat, son, dietary        | Dietary & Health Disputes            |
| 9         | support, legal, financial       | Legal Advice & Financial Contracts   |
| 10        | sister, wedding, dress, kids    | Wedding Attire & Child-free Weddings |

### Comparison: Chi-Square Test

-   **Statistical Test:** χ² = 37.93, p \< 0.00000003
-   **Contingency Table Input:**

    | Model  | True OP → OP | True OP → Other | True Other → OP | True Other → Other |
    |--------|--------------|-----------------|-----------------|--------------------|
    | Gemini | 714          | 97              | 47              | 24                 |
    | BERT   | 666          | 201             | 60              | 19                 |

-   **Results:**

    -   Chi² = 37.93\
    -   p-value = 2.92 × 10⁻⁸\
    -   DoF = 3

-   **Interpretation:**

    -   Strong evidence that BERT and Gemini behave differently
    -   Supports our argument that model choice matters in moral sentiment analysis
-   **Interpretation:** Gemini and BERT **distribute predictions significantly differently**
-   **Implication:** Model choice affects who we think is being criticized in AITA — not all LLMs “read” judgment the same way

### Confusion Matrix Heatmaps {.center}

![Confusion matrices for Gemini and BERT](heatmap.png)

-   **Gemini (left):** Stronger performance, especially for OP detection\
-   **BERT (right):** Weaker precision and recall on “Other” predictions\
-   This visual shows how both models skew toward OP classification
:::

::: notes
Charles + Daisy
Let’s break down what we found.

**Gemini** performed better overall. It correctly predicted OP criticism more often and had a higher F1-score for both classes. Its accuracy was 84%, and it handled “Other” targets better than BERT did — though even Gemini struggled with minority class predictions.

**BERT** lagged behind with a 72% accuracy. It was especially weak at catching “Other” criticism — that is, when someone in the post other than the OP was being judged.

This suggests BERT has trouble with context and social nuance. It’s likely treating all comments as if they’re about the main poster.

The heatmaps drive this home visually. Gemini’s predictions are more balanced; BERT heavily favors OP classification and misses nuance.

Together, these results show that even among advanced language models, some are better equipped to understand how people judge each other in online discourse.


This slide shows the statistical backbone of our model comparison.

The chi-square test checks whether Gemini and BERT make different types of mistakes. For example, are they both bad at identifying criticism of “Other” people? Or does one model tend to overclassify everything as OP?

We built a 2x2x2 contingency table with prediction outcomes from both models: - True OP → predicted OP - True OP → predicted Other - True Other → predicted OP - True Other → predicted Other

The results show a chi-square value of 37.93 and a p-value well below 0.000001. This means the differences between models are **highly statistically significant** — they don’t behave the same, and those differences aren’t random.

This test gives us solid evidence that Gemini and BERT interpret moral judgment in different ways.
:::

## Conclusion and Limitations {.scrollable .smaller}

::: panel-tabset
### Conclusion

-   **Key Insight:**
    -   Sentiment models like BERT and Gemini can approximate moral judgments in r/AmITheAsshole (AITA) discussions.
-   **Model Comparison:**
    -   **Gemini** outperformed BERT in identifying when criticism targets the OP.
    -   **BERT** struggled more with detecting criticism directed at others.
-   **Statistical Significance:**
    -   A chi-square test revealed significant differences in prediction distributions between the two models (χ² = 37.93, p \< 0.00000003).
-   **Implication:**
    -   The choice of language model significantly influences the interpretation of moral judgments in online discourse.

### Limitations

-   **Label Mapping:**
    -   The binary classification of "OP" vs. "Other" was derived from sentiment labels, which may not perfectly capture the target of criticism.
-   **Model Training:**
    -   BERT was used without fine-tuning on AITA-specific data, potentially limiting its effectiveness.
-   **Data Constraints:**
    -   Some responses exceeded the 512-token limit of BERT, leading to truncation and possible loss of context.
-   **Annotation Ambiguity:**
    -   The interpretation of sentiment labels as indicators of criticism direction introduces subjectivity.

### Future Directions

-   **Model Enhancement:**
    -   Fine-tune BERT on AITA-specific data to improve its understanding of nuanced moral judgments.
-   **Expanded Annotation:**
    -   Incorporate more detailed annotations to better capture the directionality of criticism.
-   **Broader Analysis:**
    -   Explore additional linguistic features and context to enhance model predictions.
:::

::: notes
William (Conclusion ), Charles (Limitation), Daisy ( Future Direction)
To conclude: both models can *approximate* moral judgment in AITA posts, but they do it differently.

Gemini outperformed BERT, especially in identifying when the OP *wasn’t* the target. That matters — because understanding misfires or conflict direction is central to interpreting social media judgment.

However, we should be cautious: - Our labels were inferred from sentiment, not explicitly labeled as “OP” or “Other.” - BERT wasn’t fine-tuned for AITA — we used it off the shelf, which limits accuracy. - Some responses were too long and had to be truncated, which may have dropped important context.

Moving forward, we’d want to: - Fine-tune models on AITA-style data - Collect more granular labels - Incorporate relational or discourse-level features to improve context sensitivity

The takeaway? Model choice matters — and understanding how models “read” judgment is vital if we want to automate or analyze online discourse responsibly.
:::

## Questions?

## Thanks !!

------------------------------------------------------------------------
